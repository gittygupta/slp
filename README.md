# slp
* max_sequence_length --> variable (dimension consumed in attention)
* Consider padding mask from bert_utils
